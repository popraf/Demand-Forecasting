{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ARIMA\n",
    "##### AutoRegressive Integrated Moving Average\n",
    "- ARIMA models are well-known for their simplicity and interpretability. They can capture various patterns in time series data, including trends and seasonality (when using SARIMA).\n",
    "- ARIMA is effective for short-term forecasting when the data shows clear autocorrelations.\n",
    "- It is widely used in academic and professional settings for demand forecasting, making it a reliable choice.\n",
    "---\n",
    "- It's a well-established method in time series forecasting, making it a good baseline model.\n",
    "- It's relatively easy to interpret, which helps in explaining the model's decisions.\n",
    "- It works well for short-term forecasts, which aligns with the 14-day requirement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries setup\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from pmdarima import auto_arima\n",
    "from tqdm import tqdm\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Load Pipeline\n",
    "The data load pipeline is almost the same as in EDA. For the purpose of demonstration, it is not a new module, but placed as a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths\n",
    "fpath_data = os.environ.get(\"FPATH_DATA\")\n",
    "fpath_dicts = os.environ.get(\"FPATH_DICTS\")\n",
    "\n",
    "def get_data_order_items():\n",
    "    # Load order items data\n",
    "    order_items = pd.read_csv(fpath_data+\"order_items.csv\")\n",
    "    # Convert shipping_limit_date to timestamps\n",
    "    order_items['shipping_limit_date'] = pd.to_datetime(order_items['shipping_limit_date'])\n",
    "    # Lets load data regarding product category, drop redundant columns, check the data, and then join the data to order_items\n",
    "    product_categories = pd.read_csv(fpath_data+\"products.csv\")\n",
    "    product_categories = product_categories[[\"product_id\", \"product_category_name\"]]\n",
    "    # Drop nulls\n",
    "    product_categories = product_categories.dropna()\n",
    "    # Map categories to english ones\n",
    "    categories_translation_data = pd.read_csv(fpath_data+\"product_category_name_translation.csv\")\n",
    "    product_categories = pd.merge(product_categories, categories_translation_data, how='left', on='product_category_name')\n",
    "    # Instead of dropping 13 rows w/o translation, use the same data as originally\n",
    "    product_categories['product_category_name_english'] = product_categories['product_category_name_english'].fillna(product_categories['product_category_name'])\n",
    "    # Join product category to orders\n",
    "    order_items = pd.merge(order_items, product_categories, how='left', on='product_id')\n",
    "    # I decided to drop those without any category prescribed as those might be incorrect data\n",
    "    order_items = order_items.dropna(subset=['product_category_name'])\n",
    "    # Join data regarding purchase timestamp - first, read data\n",
    "    orders = pd.read_csv(fpath_data+'orders.csv')\n",
    "    # Keep only relevant columns\n",
    "    orders = orders[['order_id','order_purchase_timestamp']]\n",
    "    # Check for nulls\n",
    "    orders.isnull().sum()\n",
    "    # Transform order_purchase_timestamp to a timestamp object\n",
    "    orders['order_purchase_timestamp'] = pd.to_datetime(orders['order_purchase_timestamp'])\n",
    "    # Join orders data via inner - we could do also left join, but it would need to double check for nulls\n",
    "    order_items = pd.merge(order_items, orders, how='inner', on='order_id')\n",
    "    # Drop redundant data, not related to this task\n",
    "    # As this is just a demonstration, we won't be adding information such as holidays for a different set of countries the customers are from etc\n",
    "    order_items = order_items.drop(columns=['freight_value','shipping_limit_date','product_category_name'])\n",
    "    # Add a set of information regarding sale timestamp\n",
    "    order_items['Purchase Date'] = pd.to_datetime(order_items['order_purchase_timestamp']).dt.floor('D')\n",
    "    return order_items\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_items = get_data_order_items()\n",
    "order_items.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### As ARIMA is used, excessive features engineering is not required. Therefore, at this stage, the dataframe is grouped, and sorted by."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = order_items.groupby(['product_category_name_english', 'Purchase Date']).agg(\n",
    "    sold_products_quantity=('order_item_id', 'size')\n",
    "    # total_sales_value=('price', 'sum')\n",
    ").sort_values(by=['product_category_name_english', 'Purchase Date'], ascending=True).reset_index()\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset[dataset['product_category_name_english']=='art'].tail(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset[dataset['product_category_name_english']=='agro_industry_and_commerce'].tail(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### To avoid manual search of terms such as AR, MA and checking differentiation, I use auto arima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_stationarity(df):\n",
    "    # Perform Dickey-Fuller test\n",
    "    result = adfuller(df, autolag='AIC')\n",
    "    return result[1] <= 0.05  # Return True if p-value <= 0.05 (stationary)\n",
    "\n",
    "def find_optimal_d(df):\n",
    "    d = 0\n",
    "    while not test_stationarity(df) and d < 2:\n",
    "        df = df.diff().dropna()\n",
    "        d += 1\n",
    "    return d\n",
    "\n",
    "def analyze_acf_pacf(df, d):\n",
    "    diff_series = df.diff(d).dropna()\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 8))\n",
    "    plot_acf(diff_series, ax=ax1, lags=40)\n",
    "    plot_pacf(diff_series, ax=ax2, lags=40)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wmae(y_true, y_pred):\n",
    "    return np.sum(np.abs(y_true - y_pred) * np.arange(1, len(y_true)+1)) / np.sum(np.arange(1, len(y_true)+1))\n",
    "\n",
    "def bias(y_true, y_pred):\n",
    "    return np.mean(y_pred - y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize dictionary to store forecasts and metrics to evaluate model\n",
    "forecasts = {}\n",
    "metrics = {}\n",
    "\n",
    "# Get unique categories\n",
    "categories = dataset['product_category_name_english'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sarima_forecast(train_data, test_data, forecast_end='2018-09-24'):\n",
    "    print(f\"Train data shape: {train_data.shape}\")\n",
    "    print(f\"Test data shape: {test_data.shape}\")\n",
    "    print(f\"Train data range: {train_data.min()} to {train_data.max()}\")\n",
    "    \n",
    "    if train_data.empty or train_data.isnull().all():\n",
    "        print(\"Error: Train data is empty or all null\")\n",
    "        return None, None\n",
    "\n",
    "    if train_data.nunique() == 1:\n",
    "        print(f\"Warning: All values in train data are the same ({train_data.iloc[0]}). Using naive forecast.\")\n",
    "        constant_value = train_data.iloc[0]\n",
    "        forecast_start = test_data.index[-1] + pd.Timedelta(days=1)\n",
    "        forecast_dates = pd.date_range(start=forecast_start, end=forecast_end, freq='D')\n",
    "        return (np.full(len(test_data), constant_value), \n",
    "                pd.Series(np.full(len(forecast_dates), constant_value), index=forecast_dates))\n",
    "\n",
    "    try:\n",
    "        model = auto_arima(train_data,\n",
    "                        seasonal=True,\n",
    "                        m=7,  # 7 or 365 for yearly seasonality\n",
    "                        # start_p=0, start_q=0,\n",
    "                        # max_p=5, max_q=5,\n",
    "                        # start_P=0, start_Q=0,\n",
    "                        # max_P=2, max_Q=2,\n",
    "                        # d=None, D=None,\n",
    "                        trace=True,\n",
    "                        error_action='ignore',\n",
    "                        suppress_warnings=True,\n",
    "                        stepwise=True, # Exhaustive search\n",
    "                        n_fits=100) # Increase for more thorough search        \n",
    "        print(f\"Best SARIMA order: {model.order}\")\n",
    "        print(f\"Best seasonal order: {model.seasonal_order}\")\n",
    "       \n",
    "        results = model.fit(train_data)\n",
    "       \n",
    "        in_sample_forecast = results.predict(n_periods=len(test_data))\n",
    "        print(f\"In-sample forecast shape: {in_sample_forecast.shape}\")\n",
    "        print(f\"In-sample forecast range: {in_sample_forecast.min()} to {in_sample_forecast.max()}\")\n",
    "       \n",
    "        forecast_start = test_data.index[-1] + pd.Timedelta(days=1)\n",
    "        forecast_dates = pd.date_range(start=forecast_start, end=forecast_end, freq='D')\n",
    "        out_of_sample_forecast = results.predict(n_periods=len(forecast_dates))\n",
    "        print(f\"Out-of-sample forecast shape: {out_of_sample_forecast.shape}\")\n",
    "        print(f\"Out-of-sample forecast range: {out_of_sample_forecast.min()} to {out_of_sample_forecast.max()}\")\n",
    "       \n",
    "        return in_sample_forecast, pd.Series(out_of_sample_forecast, index=forecast_dates)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error during forecasting: {str(e)}\")\n",
    "        mean_value = train_data.mean()\n",
    "        forecast_start = test_data.index[-1] + pd.Timedelta(days=1)\n",
    "        forecast_dates = pd.date_range(start=forecast_start, end=forecast_end, freq='D')\n",
    "        return (np.full(len(test_data), mean_value), \n",
    "                pd.Series(np.full(len(forecast_dates), mean_value), index=forecast_dates))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for category in tqdm(categories, desc=\"Processing categories\"):\n",
    "    print(f\"\\nAnalyzing category: {category}\")\n",
    "   \n",
    "    category_data = dataset[dataset['product_category_name_english'] == category]\n",
    "    daily_sales = category_data.groupby('Purchase Date')['sold_products_quantity'].sum()\n",
    "    daily_sales.index = pd.to_datetime(daily_sales.index)\n",
    "    daily_sales = daily_sales.resample('D').sum()\n",
    "   \n",
    "    # Fill missing dates with 0 up to the last date in the dataset\n",
    "    date_range = pd.date_range(start=daily_sales.index.min(), end=daily_sales.index.max(), freq='D')\n",
    "    daily_sales = daily_sales.reindex(date_range, fill_value=0)\n",
    "   \n",
    "    split_point = int(len(daily_sales) * 0.8)\n",
    "    train_data = daily_sales[:split_point]\n",
    "    test_data = daily_sales[split_point:]\n",
    "   \n",
    "    in_sample_forecast, out_of_sample_forecast = sarima_forecast(train_data, test_data, forecast_end='2018-09-24')\n",
    "   \n",
    "    if in_sample_forecast is None or out_of_sample_forecast is None:\n",
    "        print(f\"Skipping category {category} due to forecast failure\")\n",
    "        continue\n",
    "\n",
    "    # Replace NaN with mean value\n",
    "    mean_value = train_data.mean()\n",
    "    in_sample_forecast = np.nan_to_num(in_sample_forecast, nan=mean_value)\n",
    "    out_of_sample_forecast = out_of_sample_forecast.fillna(mean_value)\n",
    "\n",
    "    mae = mean_absolute_error(test_data, in_sample_forecast)\n",
    "    wmae_score = wmae(test_data.values, in_sample_forecast)\n",
    "    bias_score = bias(test_data.values, in_sample_forecast)\n",
    "   \n",
    "    metrics[category] = {\n",
    "        'MAE': mae,\n",
    "        'WMAE': wmae_score,\n",
    "        'Bias': bias_score\n",
    "    }\n",
    "   \n",
    "    forecasts[category] = out_of_sample_forecast\n",
    "\n",
    "# Create DataFrames with all forecasts and metrics\n",
    "forecast_df = pd.DataFrame(forecasts)\n",
    "metrics_df = pd.DataFrame(metrics).T\n",
    "\n",
    "print(\"\\nForecasts up to 2018-09-24:\")\n",
    "print(forecast_df)\n",
    "print(\"\\nMetrics for each category:\")\n",
    "print(metrics_df)\n",
    "\n",
    "forecast_df.to_csv('ARIMA_category_demand_forecasts.csv')\n",
    "metrics_df.to_csv('ARIMA_category_forecast_metrics.csv')\n",
    "print(\"Forecasts have been saved to 'ARIMA_category_demand_forecasts.csv'\")\n",
    "print(\"Metrics have been saved to 'ARIMA_category_forecast_metrics.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "demand_forecast",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
